{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.createDataFrame(rdd,schema)\n",
    "#spark.createDataFrame(rdd).toDF([list of column names])\n",
    "#rdd.toDF(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+\n",
      "|   name|age|\n",
      "+-------+---+\n",
      "|  Alice| 30|\n",
      "|    Bob| 25|\n",
      "|Charlie| 35|\n",
      "+-------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "#Step 1: Create a SparkSession\n",
    "spark = SparkSession.builder.appName(\"RDD to DataFrame example\").getOrCreate()\n",
    "\n",
    "#Step 1: Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"RDD to DataFrame example\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Step 2: Define the schema for your DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    # Add more fields as needed\n",
    "])\n",
    "\n",
    "# Step 3: Create an RDD containing your data\n",
    "data_rdd = spark.sparkContext.parallelize([\n",
    "    (\"Alice\", 30),\n",
    "    (\"Bob\", 25),\n",
    "    (\"Charlie\", 35),\n",
    "    # Add more data as needed\n",
    "])\n",
    "\n",
    "# Step 4: Convert the RDD into a DataFrame using the defined schema\n",
    "df = spark.createDataFrame(data_rdd, schema)\n",
    "\n",
    "# Show the DataFrame\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -head /user/tkm/retail_db/orders.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+---------------+\n",
      "|orderid|           orderdate|customerid|    orderstatus|\n",
      "+-------+--------------------+----------+---------------+\n",
      "|      1|2013-07-25 00:00:...|     11599|         CLOSED|\n",
      "|      2|2013-07-25 00:00:...|       256|PENDING_PAYMENT|\n",
      "|      3|2013-07-25 00:00:...|     12111|       COMPLETE|\n",
      "|      4|2013-07-25 00:00:...|      8827|         CLOSED|\n",
      "|      5|2013-07-25 00:00:...|     11318|       COMPLETE|\n",
      "|      6|2013-07-25 00:00:...|      7130|       COMPLETE|\n",
      "|      7|2013-07-25 00:00:...|      4530|       COMPLETE|\n",
      "|      8|2013-07-25 00:00:...|      2911|     PROCESSING|\n",
      "|      9|2013-07-25 00:00:...|      5657|PENDING_PAYMENT|\n",
      "|     10|2013-07-25 00:00:...|      5648|PENDING_PAYMENT|\n",
      "|     11|2013-07-25 00:00:...|       918| PAYMENT_REVIEW|\n",
      "|     12|2013-07-25 00:00:...|      1837|         CLOSED|\n",
      "|     13|2013-07-25 00:00:...|      9149|PENDING_PAYMENT|\n",
      "|     14|2013-07-25 00:00:...|      9842|     PROCESSING|\n",
      "|     15|2013-07-25 00:00:...|      2568|       COMPLETE|\n",
      "|     16|2013-07-25 00:00:...|      7276|PENDING_PAYMENT|\n",
      "|     17|2013-07-25 00:00:...|      2667|       COMPLETE|\n",
      "|     18|2013-07-25 00:00:...|      1205|         CLOSED|\n",
      "|     19|2013-07-25 00:00:...|      9488|PENDING_PAYMENT|\n",
      "|     20|2013-07-25 00:00:...|      9198|     PROCESSING|\n",
      "+-------+--------------------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDD to DataFrame example\").getOrCreate()\n",
    "\n",
    "orders_rdd = spark.sparkContext.textFile(\"hdfs://localhost:9000/user/tkm/retail_db/orders.csv\")\n",
    "\n",
    "mapped_rdd = orders_rdd.map(lambda line: line.split(\",\"))\n",
    "\n",
    "orders_schema= StructType([\n",
    "    StructField(\"orderid\",StringType()),\n",
    "    StructField(\"orderdate\",StringType()),\n",
    "    StructField(\"customerid\",StringType()),\n",
    "    StructField(\"orderstatus\",StringType())\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(mapped_rdd, orders_schema)\n",
    "\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+----------+---------------+\n",
      "|orderid|           orderdate|customerid|    orderstatus|\n",
      "+-------+--------------------+----------+---------------+\n",
      "|      1|2013-07-25 00:00:...|     11599|         CLOSED|\n",
      "|      2|2013-07-25 00:00:...|       256|PENDING_PAYMENT|\n",
      "|      3|2013-07-25 00:00:...|     12111|       COMPLETE|\n",
      "|      4|2013-07-25 00:00:...|      8827|         CLOSED|\n",
      "|      5|2013-07-25 00:00:...|     11318|       COMPLETE|\n",
      "|      6|2013-07-25 00:00:...|      7130|       COMPLETE|\n",
      "|      7|2013-07-25 00:00:...|      4530|       COMPLETE|\n",
      "|      8|2013-07-25 00:00:...|      2911|     PROCESSING|\n",
      "|      9|2013-07-25 00:00:...|      5657|PENDING_PAYMENT|\n",
      "|     10|2013-07-25 00:00:...|      5648|PENDING_PAYMENT|\n",
      "|     11|2013-07-25 00:00:...|       918| PAYMENT_REVIEW|\n",
      "|     12|2013-07-25 00:00:...|      1837|         CLOSED|\n",
      "|     13|2013-07-25 00:00:...|      9149|PENDING_PAYMENT|\n",
      "|     14|2013-07-25 00:00:...|      9842|     PROCESSING|\n",
      "|     15|2013-07-25 00:00:...|      2568|       COMPLETE|\n",
      "|     16|2013-07-25 00:00:...|      7276|PENDING_PAYMENT|\n",
      "|     17|2013-07-25 00:00:...|      2667|       COMPLETE|\n",
      "|     18|2013-07-25 00:00:...|      1205|         CLOSED|\n",
      "|     19|2013-07-25 00:00:...|      9488|PENDING_PAYMENT|\n",
      "|     20|2013-07-25 00:00:...|      9198|     PROCESSING|\n",
      "+-------+--------------------+----------+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = mapped_rdd.toDF(orders_schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDD to DataFrame example\").getOrCreate()\n",
    "\n",
    "orders_rdd = spark.sparkContext.textFile(\"hdfs://localhost:9000/user/tkm/retail_db/orders.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1,2013-07-25 00:00:00.0,11599,CLOSED',\n",
       " '2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT',\n",
       " '3,2013-07-25 00:00:00.0,12111,COMPLETE',\n",
       " '4,2013-07-25 00:00:00.0,8827,CLOSED',\n",
       " '5,2013-07-25 00:00:00.0,11318,COMPLETE']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orders_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_rdd = orders_rdd.map(lambda x :(int(x.split(\",\")[0]) ,x.split(\",\")[1] ,int(x.split(\",\")[2]) ,x.split(\",\")[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, '2013-07-25 00:00:00.0', 11599, 'CLOSED'),\n",
       " (2, '2013-07-25 00:00:00.0', 256, 'PENDING_PAYMENT'),\n",
       " (3, '2013-07-25 00:00:00.0', 12111, 'COMPLETE'),\n",
       " (4, '2013-07-25 00:00:00.0', 8827, 'CLOSED'),\n",
       " (5, '2013-07-25 00:00:00.0', 11318, 'COMPLETE')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapped_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_schema = \"order_id int ,order_date string ,customer_id int ,order_status string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:...|       8827|         CLOSED|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(mapped_rdd , order_schema)\n",
    "df.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "df_new = df.withColumn(\"order_date\" ,to_timestamp(\"order_date\"))\n",
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: timestamp (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|order_id|         order_date|customer_id|   order_status|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:00|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:00|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:00|      12111|       COMPLETE|\n",
      "|       4|2013-07-25 00:00:00|       8827|         CLOSED|\n",
      "+--------+-------------------+-----------+---------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#from pyspark.sql import SparkSession\n",
    "#spark = SparkSession.builder.appName(\"RDD to DataFrame example\").getOrCreate()\n",
    "\n",
    "orders_rdd = spark.sparkContext.textFile(\"hdfs://localhost:9000/user/tkm/retail_db/orders.csv\")\n",
    "\n",
    "mapped_rdd = orders_rdd.map(lambda x :(int(x.split(\",\")[0]) ,\n",
    "                                       x.split(\",\")[1] ,\n",
    "                                       int(x.split(\",\")[2]) ,\n",
    "                                       x.split(\",\")[3]\n",
    "                                       )\n",
    "                            )\n",
    "\n",
    "order_schema = \"order_id int ,order_date string ,customer_id int ,order_status string\"\n",
    "\n",
    "df = spark.createDataFrame(mapped_rdd , order_schema)\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "df_new = df.withColumn(\"order_date\" ,to_timestamp(\"order_date\"))\n",
    "\n",
    "df_new.printSchema()\n",
    "\n",
    "df_new.show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+-----+---------------+\n",
      "| _1|                  _2|   _3|             _4|\n",
      "+---+--------------------+-----+---------------+\n",
      "|  1|2013-07-25 00:00:...|11599|         CLOSED|\n",
      "|  2|2013-07-25 00:00:...|  256|PENDING_PAYMENT|\n",
      "|  3|2013-07-25 00:00:...|12111|       COMPLETE|\n",
      "+---+--------------------+-----+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame(mapped_rdd )\n",
    "df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df = df.toDF(\"order_id\" ,\"order_date\" ,\"customer_id\" ,\"order_status\")\n",
    "new_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- order_id: long (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: long (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_rdd = spark.sparkContext.textFile(\"hdfs://localhost:9000/user/tkm/retail_db/orders.csv\")\n",
    "\n",
    "mapped_rdd = orders_rdd.map(lambda x :(int(x.split(\",\")[0]) ,\n",
    "                                       x.split(\",\")[1] ,\n",
    "                                       int(x.split(\",\")[2]) ,\n",
    "                                       x.split(\",\")[3]\n",
    "                                       )\n",
    "                            )\n",
    "df = spark.createDataFrame(mapped_rdd )\n",
    "new_df = df.toDF(\"order_id\" ,\"order_date\" ,\"customer_id\" ,\"order_status\")\n",
    "\n",
    "new_df.show(3)\n",
    "\n",
    "new_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+-----------+---------------+\n",
      "|order_id|          order_date|customer_id|   order_status|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "|       1|2013-07-25 00:00:...|      11599|         CLOSED|\n",
      "|       2|2013-07-25 00:00:...|        256|PENDING_PAYMENT|\n",
      "|       3|2013-07-25 00:00:...|      12111|       COMPLETE|\n",
      "+--------+--------------------+-----------+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "root\n",
      " |-- order_id: integer (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- customer_id: integer (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "orders_rdd = spark.sparkContext.textFile(\"hdfs://localhost:9000/user/tkm/retail_db/orders.csv\")\n",
    "\n",
    "mapped_rdd = orders_rdd.map(lambda x :(int(x.split(\",\")[0]) ,\n",
    "                                       x.split(\",\")[1] ,\n",
    "                                       int(x.split(\",\")[2]) ,\n",
    "                                       x.split(\",\")[3]\n",
    "                                       )\n",
    "                            )\n",
    "\n",
    "order_schema = \"order_id int ,order_date string ,customer_id int ,order_status string\"\n",
    "\n",
    "df_mod = mapped_rdd.toDF(order_schema)\n",
    "\n",
    "df_mod.show(3)\n",
    "\n",
    "df_mod.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
