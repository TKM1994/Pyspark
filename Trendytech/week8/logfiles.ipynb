{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"logs\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_data = [(\"DEBUG\",\"2014-6-22 21:30:49\"),\n",
    "(\"WARN\",\"2013-12-6 17:54:15\"),\n",
    "(\"DEBUG\",\"2017-1-12 10:47:02\"),\n",
    "(\"DEBUG\",\"2016-6-25 11:06:42\"),\n",
    "(\"ERROR\",\"2015-6-28 19:25:05\"),\n",
    "(\"DEBUG\",\"2012-6-24 01:06:37\"),\n",
    "(\"INFO\",\"2014-12-9 09:53:54\"),\n",
    "(\"DEBUG\",\"2015-11-8 19:20:08\"),\n",
    "(\"INFO\",\"2017-12-21 18:34:18\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logs_df = spark.createDataFrame(logs_data).toDF(\"loglevel\",\"logtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|loglevel|            logtime|\n",
      "+--------+-------------------+\n",
      "|   DEBUG| 2014-6-22 21:30:49|\n",
      "|    WARN| 2013-12-6 17:54:15|\n",
      "|   DEBUG| 2017-1-12 10:47:02|\n",
      "|   DEBUG| 2016-6-25 11:06:42|\n",
      "|   ERROR| 2015-6-28 19:25:05|\n",
      "|   DEBUG| 2012-6-24 01:06:37|\n",
      "|    INFO| 2014-12-9 09:53:54|\n",
      "|   DEBUG| 2015-11-8 19:20:08|\n",
      "|    INFO|2017-12-21 18:34:18|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loglevel: string (nullable = true)\n",
      " |-- logtime: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "logs_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = logs_df.withColumn(\"logtime\", to_timestamp(\"logtime\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loglevel: string (nullable = true)\n",
      " |-- logtime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df.createOrReplaceTempView(\"logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|loglevel|            logtime|\n",
      "+--------+-------------------+\n",
      "|   DEBUG|2014-06-22 21:30:49|\n",
      "|    WARN|2013-12-06 17:54:15|\n",
      "|   DEBUG|2017-01-12 10:47:02|\n",
      "|   DEBUG|2016-06-25 11:06:42|\n",
      "|   ERROR|2015-06-28 19:25:05|\n",
      "|   DEBUG|2012-06-24 01:06:37|\n",
      "|    INFO|2014-12-09 09:53:54|\n",
      "|   DEBUG|2015-11-08 19:20:08|\n",
      "|    INFO|2017-12-21 18:34:18|\n",
      "+--------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from logs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+----------+----------+---------+-----------+----+----+-------+-------+------+------+---------------+---+---+----+----+------+\n",
      "|loglevel|            logtime|full_month|half_month|num_month|short_month|year|date|hour24f|hour12f|minute|second|required_format|doy|dow|ndow|ampm|quater|\n",
      "+--------+-------------------+----------+----------+---------+-----------+----+----+-------+-------+------+------+---------------+---+---+----+----+------+\n",
      "|   DEBUG|2014-06-22 21:30:49|      June|       Jun|       06|          6|2014|  22|     21|      9|    30|    49|     06/22/2014|173|Sun|   1|  PM|     2|\n",
      "|    WARN|2013-12-06 17:54:15|  December|       Dec|       12|         12|2013|   6|     17|      5|    54|    15|     12/06/2013|340|Fri|   6|  PM|     4|\n",
      "|   DEBUG|2017-01-12 10:47:02|   January|       Jan|       01|          1|2017|  12|     10|     10|    47|     2|     01/12/2017| 12|Thu|   5|  AM|     1|\n",
      "|   DEBUG|2016-06-25 11:06:42|      June|       Jun|       06|          6|2016|  25|     11|     11|     6|    42|     06/25/2016|177|Sat|   4|  AM|     2|\n",
      "|   ERROR|2015-06-28 19:25:05|      June|       Jun|       06|          6|2015|  28|     19|      7|    25|     5|     06/28/2015|179|Sun|   7|  PM|     2|\n",
      "|   DEBUG|2012-06-24 01:06:37|      June|       Jun|       06|          6|2012|  24|      1|      1|     6|    37|     06/24/2012|176|Sun|   3|  AM|     2|\n",
      "|    INFO|2014-12-09 09:53:54|  December|       Dec|       12|         12|2014|   9|      9|      9|    53|    54|     12/09/2014|343|Tue|   2|  AM|     4|\n",
      "|   DEBUG|2015-11-08 19:20:08|  November|       Nov|       11|         11|2015|   8|     19|      7|    20|     8|     11/08/2015|312|Sun|   1|  PM|     4|\n",
      "|    INFO|2017-12-21 18:34:18|  December|       Dec|       12|         12|2017|  21|     18|      6|    34|    18|     12/21/2017|355|Thu|   7|  PM|     4|\n",
      "+--------+-------------------+----------+----------+---------+-----------+----+----+-------+-------+------+------+---------------+---+---+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select loglevel ,logtime,\n",
    "          date_format(logtime , 'MMMM') as full_month,\n",
    "          date_format(logtime , 'MMM') as half_month,\n",
    "          date_format(logtime , 'MM') as num_month,\n",
    "          date_format(logtime , 'M') as short_month,\n",
    "          date_format(logtime , 'y') as year,\n",
    "          date_format(logtime , 'd') as date,\n",
    "          date_format(logtime , 'H') as hour24f,\n",
    "          date_format(logtime , 'h') as hour12f,\n",
    "          date_format(logtime , 'm') as minute,\n",
    "          date_format(logtime , 's') as second,\n",
    "          date_format(logtime , 'MM/dd/yyy') as required_format,\n",
    "          date_format(logtime , 'D') as doy,\n",
    "          date_format(logtime , 'E') as dow,\n",
    "          date_format(logtime , 'F') as ndow,\n",
    "          date_format(logtime , 'a') as ampm,\n",
    "          date_format(logtime , 'q') as quater\n",
    "          from logs\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+---+\n",
      "|loglevel|            logtime|new|\n",
      "+--------+-------------------+---+\n",
      "|   DEBUG|2014-06-22 21:30:49|  2|\n",
      "|    WARN|2013-12-06 17:54:15|  4|\n",
      "|   DEBUG|2017-01-12 10:47:02|  1|\n",
      "|   DEBUG|2016-06-25 11:06:42|  2|\n",
      "|   ERROR|2015-06-28 19:25:05|  2|\n",
      "|   DEBUG|2012-06-24 01:06:37|  2|\n",
      "|    INFO|2014-12-09 09:53:54|  4|\n",
      "|   DEBUG|2015-11-08 19:20:08|  4|\n",
      "|    INFO|2017-12-21 18:34:18|  4|\n",
      "+--------+-------------------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select loglevel ,date_format(logtime , 'MMMM') as month from logs\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+\n",
      "|   month|loglevel|total_occurance|\n",
      "+--------+--------+---------------+\n",
      "|December|    WARN|              1|\n",
      "|December|    INFO|              2|\n",
      "| January|   DEBUG|              1|\n",
      "|    June|   DEBUG|              3|\n",
      "|    June|   ERROR|              1|\n",
      "|November|   DEBUG|              1|\n",
      "+--------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          select \n",
    "          date_format(logtime ,\"MMMM\") as month,\n",
    "          loglevel,\n",
    "          count(*) as total_occurance\n",
    "          from logs\n",
    "          group by month,loglevel\n",
    "          order by month asc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+---------------+\n",
      "|   month|month_number|loglevel|total_occurance|\n",
      "+--------+------------+--------+---------------+\n",
      "| January|           1|   DEBUG|              1|\n",
      "|November|          11|   DEBUG|              1|\n",
      "|December|          12|    WARN|              1|\n",
      "|December|          12|    INFO|              2|\n",
      "|    June|           6|   DEBUG|              3|\n",
      "|    June|           6|   ERROR|              1|\n",
      "+--------+------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#M groupby without casting not expected op\n",
    "spark.sql(\"\"\"\n",
    "          select \n",
    "          date_format(logtime ,\"MMMM\") as month,\n",
    "          date_format(logtime ,\"M\") as month_number,\n",
    "          loglevel,\n",
    "          count(*) as total_occurance\n",
    "          from logs\n",
    "          group by month,month_number,loglevel\n",
    "          order by month_number asc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+---------------+\n",
      "|   month|month_number|loglevel|total_occurance|\n",
      "+--------+------------+--------+---------------+\n",
      "| January|           1|   DEBUG|              1|\n",
      "|November|          11|   DEBUG|              1|\n",
      "|December|          12|    INFO|              2|\n",
      "|December|          12|    WARN|              1|\n",
      "|    June|           6|   DEBUG|              3|\n",
      "|    June|           6|   ERROR|              1|\n",
      "+--------+------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#M max without casting not expected op\n",
    "spark.sql(\"\"\"\n",
    "          select \n",
    "          date_format(logtime ,\"MMMM\") as month,\n",
    "          max(date_format(logtime ,\"M\")) as month_number,\n",
    "          loglevel,\n",
    "          count(*) as total_occurance\n",
    "          from logs\n",
    "          group by month,loglevel\n",
    "          order by month_number asc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+---------------+\n",
      "|   month|month_number|loglevel|total_occurance|\n",
      "+--------+------------+--------+---------------+\n",
      "| January|           1|   DEBUG|              1|\n",
      "|    June|           6|   DEBUG|              3|\n",
      "|    June|           6|   ERROR|              1|\n",
      "|November|          11|   DEBUG|              1|\n",
      "|December|          12|    WARN|              1|\n",
      "|December|          12|    INFO|              2|\n",
      "+--------+------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#M groupby  with cast\n",
    "spark.sql(\"\"\"\n",
    "          select \n",
    "          date_format(logtime ,\"MMMM\") as month,\n",
    "          int(date_format(logtime ,\"M\")) as month_number,\n",
    "          loglevel,\n",
    "          count(*) as total_occurance\n",
    "          from logs\n",
    "          group by month,month_number,loglevel\n",
    "          order by month_number asc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+---------------+\n",
      "|   month|month_number|loglevel|total_occurance|\n",
      "+--------+------------+--------+---------------+\n",
      "| January|           1|   DEBUG|              1|\n",
      "|    June|           6|   DEBUG|              3|\n",
      "|    June|           6|   ERROR|              1|\n",
      "|November|          11|   DEBUG|              1|\n",
      "|December|          12|    WARN|              1|\n",
      "|December|          12|    INFO|              2|\n",
      "+--------+------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#M max with int cast\n",
    "spark.sql(\"\"\"\n",
    "          select \n",
    "          date_format(logtime ,\"MMMM\") as month,\n",
    "          max(int(date_format(logtime ,\"M\"))) as month_number,\n",
    "          loglevel,\n",
    "          count(*) as total_occurance\n",
    "          from logs\n",
    "          group by month,loglevel\n",
    "          order by month_number asc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+---------------+\n",
      "|   month|month_number|loglevel|total_occurance|\n",
      "+--------+------------+--------+---------------+\n",
      "| January|          01|   DEBUG|              1|\n",
      "|    June|          06|   DEBUG|              3|\n",
      "|    June|          06|   ERROR|              1|\n",
      "|November|          11|   DEBUG|              1|\n",
      "|December|          12|    INFO|              2|\n",
      "|December|          12|    WARN|              1|\n",
      "+--------+------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MM max without cast\n",
    "spark.sql(\"\"\"\n",
    "          select \n",
    "          date_format(logtime ,\"MMMM\") as month,\n",
    "          max(date_format(logtime ,\"MM\")) as month_number,\n",
    "          loglevel,\n",
    "          count(*) as total_occurance\n",
    "          from logs\n",
    "          group by month,loglevel\n",
    "          order by month_number asc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+---------------+\n",
      "|   month|month_number|loglevel|total_occurance|\n",
      "+--------+------------+--------+---------------+\n",
      "| January|          01|   DEBUG|              1|\n",
      "|    June|          06|   DEBUG|              3|\n",
      "|    June|          06|   ERROR|              1|\n",
      "|November|          11|   DEBUG|              1|\n",
      "|December|          12|    WARN|              1|\n",
      "|December|          12|    INFO|              2|\n",
      "+--------+------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#MM groupby without cast\n",
    "spark.sql(\"\"\"\n",
    "          select \n",
    "          date_format(logtime ,\"MMMM\") as month,\n",
    "          date_format(logtime ,\"MM\") as month_number,\n",
    "          loglevel,\n",
    "          count(*) as total_occurance\n",
    "          from logs\n",
    "          group by month,month_number,loglevel\n",
    "          order by month_number asc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+---------------+\n",
      "|   month|month_number|loglevel|total_occurance|\n",
      "+--------+------------+--------+---------------+\n",
      "| January|          01|   DEBUG|              1|\n",
      "|    June|          06|   DEBUG|              3|\n",
      "|    June|          06|   ERROR|              1|\n",
      "|November|          11|   DEBUG|              1|\n",
      "|December|          12|    INFO|              2|\n",
      "|December|          12|    WARN|              1|\n",
      "+--------+------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          select \n",
    "          date_format(logtime ,\"MMMM\") as month,\n",
    "          first(date_format(logtime ,\"MM\")) as month_number,\n",
    "          loglevel,\n",
    "          count(*) as total_occurance\n",
    "          from logs\n",
    "          group by month,loglevel\n",
    "          order by month_number asc\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+--------+---------------+\n",
      "|   month|month_number|loglevel|total_occurance|\n",
      "+--------+------------+--------+---------------+\n",
      "| January|          01|   DEBUG|              1|\n",
      "|    June|          06|   DEBUG|              3|\n",
      "|    June|          06|   ERROR|              1|\n",
      "|November|          11|   DEBUG|              1|\n",
      "|December|          12|    INFO|              2|\n",
      "|December|          12|    WARN|              1|\n",
      "+--------+------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results_df = spark.sql(\"\"\"\n",
    "                    select \n",
    "                    date_format(logtime ,\"MMMM\") as month,\n",
    "                    first(date_format(logtime ,\"MM\")) as month_number,\n",
    "                    loglevel,\n",
    "                    count(*) as total_occurance\n",
    "                    from logs\n",
    "                    group by month,loglevel\n",
    "                    order by month_number asc\n",
    "                    \"\"\")\n",
    "\n",
    "results_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+---------------+\n",
      "|   month|loglevel|total_occurance|\n",
      "+--------+--------+---------------+\n",
      "| January|   DEBUG|              1|\n",
      "|    June|   DEBUG|              3|\n",
      "|    June|   ERROR|              1|\n",
      "|November|   DEBUG|              1|\n",
      "|December|    INFO|              2|\n",
      "|December|    WARN|              1|\n",
      "+--------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_df = results_df.drop(\"month_number\")\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
